{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bbc.mtx` looks like this:\n",
    "\n",
    "```\n",
    "%%MatrixMarket matrix coordinate real general\n",
    "9635 2225 286774\n",
    "1 1 1.0\n",
    "1 7 2.0\n",
    "1 11 1.0\n",
    "1 14 1.0\n",
    "1 15 2.0\n",
    "1 19 2.0\n",
    "...\n",
    "```\n",
    "\n",
    "Together with the `mtx` extension, this should be a clue that `MatrixMarket` is a file format. A quick Google search turns up the `mmread` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mmread('bbc.mtx')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so `m` has 9635 rows, one for each term in `bbc.terms`. And it has 2225 columns, each corresponding to a document listed in `bbc.docs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get going with NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5, init='random', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = nmf.fit_transform(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed `w` is a relationship between *terms* and *topics*. The implementation is \"thinking\" of the terms as the important data; to it the documents are just a coordinate system for expressing them.\n",
    "\n",
    "But this is just an implementation quirk. Mathematically, there's no preferential treatment to rows vs columns. And we can easily get to the \"other half\" of the decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nmf.components_\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is an approximation, fitting more closely as the number of components increases. Let's have a look at how closely we fit in this case. First we can recover an estimate of the original `m` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhat = w.dot(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a plot of predicted vs residual values, just as we do for regression. The matrix is pretty big, but we can still do this easily for the nonzeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "i,j,v = sparse.find(m) \n",
    "vhat = [mhat[ii,jj] for (ii,jj) in zip(i,j)]\n",
    "plt.scatter(vhat, v-vhat, alpha=0.1)\n",
    "plt.xlabel(\"NMF Approximated value\")\n",
    "plt.ylabel(\"Residual\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = w.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF gave us 5 topics; let's see if we can summarize these in terms of the vocabulary terms. First we'll read in the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bbc.terms') as f:\n",
    "     terms = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an annoying `newline` on the end of each, but that's easy to get rid of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [t.split()[0] for t in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for r in w.T:\n",
    "    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:7]\n",
    "    topic_words.append([terms[e[1]] for e in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data had 5 topics, as listed in `bbc.docs`. \n",
    "\n",
    "```\n",
    "Business\n",
    "Entertainment\n",
    "Politics\n",
    "Sport\n",
    "Tech\n",
    "```\n",
    "\n",
    "In \"real life\", we would have found a way to use these to inform the model. But for this little demo, we can just compare the recovered topics to the original ones. And they seem to match reasonably well. The order is different, which is to be expected in this kind of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
