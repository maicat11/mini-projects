{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we design a new machine learning algorithm from scratch. This algorithm learns how to correct for the mistakes it's made in the past by training a series of \"base learners\" one by one.\n",
    "\n",
    "1) Load the california housing data and do a train-test split as below:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.datasets import california_housing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    housing_dataset = california_housing.fetch_california_housing()\n",
    "    X = pd.DataFrame(housing_dataset.data)\n",
    "    X.columns = housing_dataset.feature_names\n",
    "    y = housing_dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)\n",
    "    \n",
    "        \n",
    "2) Ok that wasn't hard. Fit a simple linear regression on train and score it on test as a baseline.\n",
    "\n",
    "3) That also wasn't so bad. Now the hard part: create a python class or series of functions that follows the steps below to create a predictive model from training data **X, y and a hyperparameter n_estimators**.\n",
    "\n",
    "**A.** Set C = mean of y. This is our initial prediction, a constant prediction; track the current residuals y_i - C for all the target values\n",
    "\n",
    "**B.** Do the following n_estimators times: using sklearn DecisionTreeRegressor, fit a tree of max_depth 3 to (X, current residuals). Save the tree in a list, and update the residuals by subtracting the tree's predicted values on X from the current residuals.\n",
    "\n",
    "**C.** To make predictions on new data, you must sum the predictions made by all of the trees in your list, then add C. Fit your model on the training data and predict on the test data. Score your model on the test data. Try to get above .70 R^2. N_estimators = 10 is a good starting point to try.\n",
    "\n",
    "**D.** Time permitting, expand your model by adding hyperparameters **max_depth** that adjust the max_depth of each tree, as well as **learning_rate**. With learning rate, when you update the residuals subtract learning_rate * tree.predict(X) (what does this remind you of?) Also, when predicting multiply the predictions made by each tree by the learning_rate.\n",
    "\n",
    "Why do you think this works well? Where have we seen iterative mistake corrections with small step sizes come up before? Can you push your model to do even better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_dataset = california_housing.fetch_california_housing()\n",
    "X = pd.DataFrame(housing_dataset.data)\n",
    "X.columns = housing_dataset.feature_names\n",
    "y = housing_dataset.target\n",
    "\n",
    "#Split data into 3: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need these to help construct the model\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class IAmTheOneWhoBoosts():\n",
    "    \n",
    "    def __init__(self, n_estimators=10, max_depth=3, learning_rate=1):\n",
    "        \n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.C = np.mean(y)\n",
    "        self.estimators = []\n",
    "        \n",
    "        resids = y - self.C\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            \n",
    "            est = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            est.fit(X, resids)\n",
    "            resids -= self.learning_rate * est.predict(X) \n",
    "        \n",
    "            self.estimators.append(est)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return self.C + np.sum([self.learning_rate * est.predict(X) \\\n",
    "                                for est in self.estimators], axis=0)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "    \n",
    "        return r2_score(self.predict(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = IAmTheOneWhoBoosts(n_estimators=100, max_depth=3, learning_rate=.71)\n",
    "booster.fit(X_train, y_train)\n",
    "booster.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=30, max_depth=3,\n",
    "                               learning_rate=1)\n",
    "gb.fit(X_train, y_train)\n",
    "gb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that learning rates are typically lower than 1, more like 0.01-0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
