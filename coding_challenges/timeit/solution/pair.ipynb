{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is the same as merge in Pandas and inner join in SQL. One soluton: loop through list1, and for each element in list1 you loop through every element in list2. That's a double for loop and the complexity of such a code would be **O(N^2)**.\n",
    "\n",
    "Instead we could build a dictionary for list1. Then we will loop through the elements of list2 and join that way. The complexity would be **O(N)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_join(list1, list2):\n",
    "    dict1 = {item[0]:item[1:] for item in list1}\n",
    "    result = list()\n",
    "    \n",
    "    for item in list2:\n",
    "        if item[0] in dict1:\n",
    "            result.append([item[0]] + dict1[item[0]] + item[1:])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general though, the dictionary building part is done prior to running the join/merge query. That step is called **indexing**. A dictionary type index is called **hash table index**. There are other indexing used, like b-tree. Each has its advantages and disadvantages. We'll be looking at those in the coming weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_join(list1, list2):\n",
    "    dict1 = {item[0]:item[1:] for item in list1}\n",
    "    dict2 = {item[0]:item[1:] for item in list2}\n",
    "    \n",
    "    result = list()\n",
    "    \n",
    "    for key, value in dict1.items():\n",
    "        if key in dict2:\n",
    "            result.append([key] + dict1[key] + dict2[key])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a little routine to test the performance of the functions. You can use it to test your code. Try varying out the lists, dictionaries and list comprehensions and see what the difference in performance is. Of course, it will also depend on the speed of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "test1 = [[index] + [random.random() for _ in range(100)] for index in range(1000)]\n",
    "test2 = [[index] + [random.random() for _ in range(100)] for index in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit my_join(test1, test2)\n",
    "%timeit inner_join(test1, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
